INFO: [guild] running load-data: load-data balance_strat=downsample label_strat=binary ratio=0 size=1000
Resolving file:az_tk_data.csv
Resolving file:test_data.csv
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] Final Distribution of Labels: Counter({0: 125, 1: 125}) 
INFO: [guild] running train: train epochs=3 label_strat=binary lr=1.0e-05 model=distilbert warmup=0
Resolving load-data
Using run 3c141926f0c94d10ad849c0f9be4d4c5 for load-data resource
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] num_labels: 2
INFO: [root] Detected label names: ['Auszubildende', 'Sonstige Arbeitnehmer']
Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/1 [00:00<?, ?ba/s]  0%|          | 0/1 [00:00<?, ?ba/s]
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--precision\4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Mon Dec 19 10:49:43 2022) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--recall\e40e6e98d18ff3f210f4d0b26fa721bfaa80704b1fdf890fa551cfabf94fc185 (last modified on Mon Dec 19 10:49:44 2022) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--accuracy\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Dec 19 10:49:46 2022) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--f1\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Mon Dec 19 10:49:47 2022) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, __index_level_0__, text, label_class. If id, __index_level_0__, text, label_class are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 175
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 66
  Number of trainable parameters = 67007234
  0%|          | 0/66 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  2%|1         | 1/66 [00:00<00:39,  1.64it/s]  3%|3         | 2/66 [00:00<00:20,  3.17it/s]  5%|4         | 3/66 [00:00<00:14,  4.38it/s]  6%|6         | 4/66 [00:00<00:11,  5.51it/s]  8%|7         | 5/66 [00:01<00:09,  6.43it/s]  9%|9         | 6/66 [00:01<00:08,  7.16it/s] 11%|#         | 7/66 [00:01<00:07,  7.41it/s] 12%|#2        | 8/66 [00:01<00:07,  7.89it/s] 14%|#3        | 9/66 [00:01<00:06,  8.23it/s] 15%|#5        | 10/66 [00:01<00:06,  8.16it/s] 17%|#6        | 11/66 [00:01<00:06,  8.44it/s] 18%|#8        | 12/66 [00:01<00:06,  8.64it/s] 20%|#9        | 13/66 [00:01<00:06,  8.44it/s] 21%|##1       | 14/66 [00:02<00:06,  8.63it/s] 23%|##2       | 15/66 [00:02<00:05,  8.78it/s] 24%|##4       | 16/66 [00:02<00:05,  8.88it/s] 26%|##5       | 17/66 [00:02<00:05,  8.59it/s] 27%|##7       | 18/66 [00:02<00:05,  8.75it/s] 29%|##8       | 19/66 [00:02<00:05,  8.51it/s] 30%|###       | 20/66 [00:02<00:05,  8.69it/s] 32%|###1      | 21/66 [00:02<00:05,  8.81it/s] 33%|###3      | 22/66 [00:03<00:04,  8.91it/s]                                               {'loss': 0.688, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}
 33%|###3      | 22/66 [00:03<00:04,  8.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, __index_level_0__, text, label_class. If id, __index_level_0__, text, label_class are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 75
  Batch size = 8

  0%|          | 0/10 [00:00<?, ?it/s][A
 40%|####      | 4/10 [00:00<00:00, 36.58it/s][A
 80%|########  | 8/10 [00:00<00:00, 29.21it/s][A
                                              [A                                               
{'eval_loss': 0.6699086427688599, 'eval_precision': 0.5849056603773585, 'eval_recall': 0.9393939393939394, 'eval_accuracy': 0.68, 'eval_f1': 0.7209302325581395, 'eval_runtime': 0.3613, 'eval_samples_per_second': 207.607, 'eval_steps_per_second': 27.681, 'epoch': 1.0}
100%|##########| 10/10 [00:00<00:00, 29.21it/s][A 33%|###3      | 22/66 [00:03<00:04,  8.91it/s]
                                               [A 35%|###4      | 23/66 [00:03<00:09,  4.46it/s] 36%|###6      | 24/66 [00:03<00:07,  5.27it/s] 38%|###7      | 25/66 [00:03<00:06,  6.04it/s] 39%|###9      | 26/66 [00:03<00:06,  6.52it/s] 41%|####      | 27/66 [00:03<00:05,  7.13it/s] 42%|####2     | 28/66 [00:04<00:04,  7.63it/s] 44%|####3     | 29/66 [00:04<00:04,  7.74it/s] 45%|####5     | 30/66 [00:04<00:04,  8.11it/s] 47%|####6     | 31/66 [00:04<00:04,  8.40it/s] 48%|####8     | 32/66 [00:04<00:04,  8.27it/s] 50%|#####     | 33/66 [00:04<00:03,  8.52it/s] 52%|#####1    | 34/66 [00:04<00:03,  8.35it/s] 53%|#####3    | 35/66 [00:04<00:03,  8.58it/s] 55%|#####4    | 36/66 [00:04<00:03,  8.74it/s] 56%|#####6    | 37/66 [00:05<00:03,  8.86it/s] 58%|#####7    | 38/66 [00:05<00:03,  8.58it/s] 59%|#####9    | 39/66 [00:05<00:03,  8.74it/s] 61%|######    | 40/66 [00:05<00:02,  8.86it/s] 62%|######2   | 41/66 [00:05<00:02,  8.58it/s] 64%|######3   | 42/66 [00:05<00:02,  8.73it/s] 65%|######5   | 43/66 [00:05<00:02,  8.85it/s] 67%|######6   | 44/66 [00:05<00:02,  8.94it/s]                                               {'loss': 0.6394, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.0} 67%|######6   | 44/66 [00:05<00:02,  8.94it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, __index_level_0__, text, label_class. If id, __index_level_0__, text, label_class are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 75
  Batch size = 8

  0%|          | 0/10 [00:00<?, ?it/s][A
 40%|####      | 4/10 [00:00<00:00, 36.58it/s][A
 80%|########  | 8/10 [00:00<00:00, 31.31it/s][A{'eval_loss': 0.6293078660964966, 'eval_precision': 0.7777777777777778, 'eval_recall': 0.8484848484848485, 'eval_accuracy': 0.8266666666666667, 'eval_f1': 0.8115942028985507, 'eval_runtime': 0.3593, 'eval_samples_per_second': 208.726, 'eval_steps_per_second': 27.83, 'epoch': 2.0}
                                               
                                              [A 67%|######6   | 44/66 [00:06<00:02,  8.94it/s]
100%|##########| 10/10 [00:00<00:00, 31.31it/s][A
                                               [A 68%|######8   | 45/66 [00:06<00:04,  4.47it/s] 70%|######9   | 46/66 [00:06<00:03,  5.28it/s] 71%|#######1  | 47/66 [00:06<00:03,  6.05it/s] 73%|#######2  | 48/66 [00:06<00:02,  6.53it/s] 74%|#######4  | 49/66 [00:06<00:02,  7.14it/s] 76%|#######5  | 50/66 [00:06<00:02,  7.63it/s] 77%|#######7  | 51/66 [00:07<00:01,  7.74it/s] 79%|#######8  | 52/66 [00:07<00:01,  8.11it/s] 80%|########  | 53/66 [00:07<00:01,  8.40it/s] 82%|########1 | 54/66 [00:07<00:01,  8.27it/s] 83%|########3 | 55/66 [00:07<00:01,  8.52it/s] 85%|########4 | 56/66 [00:07<00:01,  8.70it/s] 86%|########6 | 57/66 [00:07<00:01,  8.47it/s] 88%|########7 | 58/66 [00:07<00:00,  8.67it/s] 89%|########9 | 59/66 [00:07<00:00,  8.45it/s] 91%|######### | 60/66 [00:08<00:00,  8.65it/s] 92%|#########2| 61/66 [00:08<00:00,  8.79it/s] 94%|#########3| 62/66 [00:08<00:00,  8.53it/s] 95%|#########5| 63/66 [00:08<00:00,  8.70it/s] 97%|#########6| 64/66 [00:08<00:00,  8.83it/s] 98%|#########8| 65/66 [00:08<00:00,  8.92it/s]100%|##########| 66/66 [00:08<00:00,  8.99it/s]                                               {'loss': 0.61, 'learning_rate': 0.0, 'epoch': 3.0}100%|##########| 66/66 [00:08<00:00,  8.99it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, __index_level_0__, text, label_class. If id, __index_level_0__, text, label_class are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 75
  Batch size = 8

  0%|          | 0/10 [00:00<?, ?it/s][A
 40%|####      | 4/10 [00:00<00:00, 36.60it/s][A
 80%|########  | 8/10 [00:00<00:00, 31.28it/s][A                                               
                                              [A{'eval_loss': 0.6056302189826965, 'eval_precision': 0.9285714285714286, 'eval_recall': 0.7878787878787878, 'eval_accuracy': 0.88, 'eval_f1': 0.8524590163934426, 'eval_runtime': 0.3595, 'eval_samples_per_second': 208.644, 'eval_steps_per_second': 27.819, 'epoch': 3.0}
100%|##########| 66/66 [00:09<00:00,  8.99it/s]
100%|##########| 10/10 [00:00<00:00, 31.28it/s][A
                                               [A

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 9.1403, 'train_samples_per_second': 57.438, 'train_steps_per_second': 7.221, 'train_loss': 0.6458053010882754, 'epoch': 3.0}
100%|##########| 66/66 [00:09<00:00,  8.99it/s]100%|##########| 66/66 [00:09<00:00,  7.23it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, __index_level_0__, text, label_class. If id, __index_level_0__, text, label_class are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 75
  Batch size = 8
  0%|          | 0/10 [00:00<?, ?it/s] 40%|####      | 4/10 [00:00<00:00, 36.57it/s] 80%|########  | 8/10 [00:00<00:00, 29.21it/s]100%|##########| 10/10 [00:00<00:00, 29.62it/s]
INFO: [root] ***** Final Eval *****
 {'eval_loss': 0.6056302189826965, 'eval_precision': 0.9285714285714286, 'eval_recall': 0.7878787878787878, 'eval_accuracy': 0.88, 'eval_f1': 0.8524590163934426, 'eval_runtime': 0.3594, 'eval_samples_per_second': 208.657, 'eval_steps_per_second': 27.821, 'epoch': 3.0}
Saving model checkpoint to C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model
Configuration saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\config.json
Model weights saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\pytorch_model.bin
tokenizer config file saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\tokenizer_config.json
Special tokens file saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\special_tokens_map.json
INFO: [guild] running test: test label_strat=binary
Resolving file:test_data.csv
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] Detected label names: ['Auszubildende', 'Sonstige Arbeitnehmer']
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--precision\4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Mon Dec 19 10:49:43 2022) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--recall\e40e6e98d18ff3f210f4d0b26fa721bfaa80704b1fdf890fa551cfabf94fc185 (last modified on Mon Dec 19 10:49:44 2022) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--accuracy\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Dec 19 10:49:46 2022) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--f1\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Mon Dec 19 10:49:47 2022) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.
all_f1: 0.8809523809523809
all_recall: 0.925
all_precision: 0.8409090909090909
all_accuracy: 0.875
tk_f1: 0.8809523809523809
tk_recall: 0.925
tk_precision: 0.8409090909090909
tk_accuracy: 0.875
ba_f1: 0.9268292682926829
ba_recall: 0.95
ba_precision: 0.9047619047619048
ba_accuracy: 0.925
len_f1: 0.8799999999999999
len_recall: 0.9166666666666666
len_precision: 0.8461538461538461
len_accuracy: 0.88
