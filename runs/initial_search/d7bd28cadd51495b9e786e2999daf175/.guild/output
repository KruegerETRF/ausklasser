INFO: [guild] running load-data: load-data balance_strat=downsample label_strat=multiclass ratio=0.5 size=1000
Resolving file:az_tk_data.csv
Resolving file:test_data.csv
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] Final Distribution of Labels: Counter({0: 125, 1: 125, 2: 125, 3: 125}) 
INFO: [guild] running train: train epochs=3 label_strat=multiclass lr=1.0e-06 model=distilbert warmup=500
Resolving load-data
Using run 4d7d4cdd605244d98ffe801cf312c515 for load-data resource
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] num_labels: 4
INFO: [root] Detected label names: ['Auszubildende', 'Verschiedenes', 'Führungskräfte', 'Fach- und Arbeitskräfte']
Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/1 [00:00<?, ?ba/s]  0%|          | 0/1 [00:00<?, ?ba/s]
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--precision\4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Mon Dec 19 10:49:43 2022) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--recall\e40e6e98d18ff3f210f4d0b26fa721bfaa80704b1fdf890fa551cfabf94fc185 (last modified on Mon Dec 19 10:49:44 2022) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--accuracy\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Dec 19 10:49:46 2022) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--f1\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Mon Dec 19 10:49:47 2022) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, label_class, text, __index_level_0__. If id, label_class, text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 350
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 132
  Number of trainable parameters = 67008772
  0%|          | 0/132 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  1%|          | 1/132 [00:00<01:24,  1.56it/s]  2%|1         | 2/132 [00:00<00:42,  3.05it/s]  2%|2         | 3/132 [00:00<00:30,  4.25it/s]  3%|3         | 4/132 [00:01<00:24,  5.21it/s]  4%|3         | 5/132 [00:01<00:21,  5.96it/s]  5%|4         | 6/132 [00:01<00:18,  6.76it/s]  5%|5         | 7/132 [00:01<00:17,  7.12it/s]  6%|6         | 8/132 [00:01<00:16,  7.38it/s]  7%|6         | 9/132 [00:01<00:16,  7.56it/s]  8%|7         | 10/132 [00:01<00:15,  7.98it/s]  8%|8         | 11/132 [00:01<00:15,  7.99it/s]  9%|9         | 12/132 [00:01<00:14,  8.24it/s] 10%|9         | 13/132 [00:02<00:14,  8.22it/s] 11%|#         | 14/132 [00:02<00:14,  8.15it/s] 11%|#1        | 15/132 [00:02<00:13,  8.43it/s] 12%|#2        | 16/132 [00:02<00:13,  8.29it/s] 13%|#2        | 17/132 [00:02<00:13,  8.54it/s] 14%|#3        | 18/132 [00:02<00:13,  8.71it/s] 14%|#4        | 19/132 [00:02<00:12,  8.83it/s] 15%|#5        | 20/132 [00:02<00:13,  8.57it/s] 16%|#5        | 21/132 [00:03<00:12,  8.73it/s] 17%|#6        | 22/132 [00:03<00:12,  8.50it/s] 17%|#7        | 23/132 [00:03<00:12,  8.68it/s] 18%|#8        | 24/132 [00:03<00:12,  8.81it/s] 19%|#8        | 25/132 [00:03<00:12,  8.55it/s] 20%|#9        | 26/132 [00:03<00:12,  8.72it/s] 20%|##        | 27/132 [00:03<00:12,  8.49it/s] 21%|##1       | 28/132 [00:03<00:11,  8.67it/s] 22%|##1       | 29/132 [00:03<00:11,  8.81it/s] 23%|##2       | 30/132 [00:04<00:11,  8.55it/s] 23%|##3       | 31/132 [00:04<00:11,  8.72it/s] 24%|##4       | 32/132 [00:04<00:11,  8.49it/s] 25%|##5       | 33/132 [00:04<00:11,  8.68it/s] 26%|##5       | 34/132 [00:04<00:11,  8.81it/s] 27%|##6       | 35/132 [00:04<00:11,  8.55it/s] 27%|##7       | 36/132 [00:04<00:11,  8.71it/s] 28%|##8       | 37/132 [00:04<00:10,  8.84it/s] 29%|##8       | 38/132 [00:04<00:10,  8.57it/s] 30%|##9       | 39/132 [00:05<00:10,  8.73it/s] 30%|###       | 40/132 [00:05<00:10,  8.50it/s] 31%|###1      | 41/132 [00:05<00:10,  8.68it/s] 32%|###1      | 42/132 [00:05<00:10,  8.82it/s] 33%|###2      | 43/132 [00:05<00:10,  8.56it/s]                                                {'loss': 1.3982, 'learning_rate': 8.8e-08, 'epoch': 1.0} 33%|###3      | 44/132 [00:05<00:10,  8.56it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, label_class, text, __index_level_0__. If id, label_class, text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 150
  Batch size = 8

  0%|          | 0/19 [00:00<?, ?it/s][A
 21%|##1       | 4/19 [00:00<00:00, 32.00it/s][A
 42%|####2     | 8/19 [00:00<00:00, 29.81it/s][A
 58%|#####7    | 11/19 [00:00<00:00, 28.81it/s][A
 74%|#######3  | 14/19 [00:00<00:00, 28.30it/s][A
 89%|########9 | 17/19 [00:00<00:00, 28.00it/s][A                                                
                                               [A{'eval_loss': 1.4004712104797363, 'eval_precision': 0.15645483043602165, 'eval_recall': 0.306959706959707, 'eval_accuracy': 0.29333333333333333, 'eval_f1': 0.18250377073906487, 'eval_runtime': 0.7034, 'eval_samples_per_second': 213.259, 'eval_steps_per_second': 27.013, 'epoch': 1.0}
 33%|###3      | 44/132 [00:06<00:10,  8.56it/s]
100%|##########| 19/19 [00:00<00:00, 28.00it/s][A
                                               [A 34%|###4      | 45/132 [00:06<00:23,  3.63it/s] 35%|###4      | 46/132 [00:06<00:20,  4.26it/s] 36%|###5      | 47/132 [00:06<00:17,  4.86it/s] 36%|###6      | 48/132 [00:06<00:15,  5.57it/s] 37%|###7      | 49/132 [00:06<00:13,  6.25it/s] 38%|###7      | 50/132 [00:07<00:12,  6.67it/s] 39%|###8      | 51/132 [00:07<00:11,  7.23it/s] 39%|###9      | 52/132 [00:07<00:10,  7.44it/s] 40%|####      | 53/132 [00:07<00:10,  7.76it/s] 41%|####      | 54/132 [00:07<00:09,  8.24it/s] 42%|####1     | 55/132 [00:07<00:09,  8.17it/s] 42%|####2     | 56/132 [00:07<00:09,  8.44it/s] 43%|####3     | 57/132 [00:07<00:09,  8.30it/s] 44%|####3     | 58/132 [00:07<00:08,  8.54it/s] 45%|####4     | 59/132 [00:08<00:08,  8.71it/s] 45%|####5     | 60/132 [00:08<00:08,  8.49it/s] 46%|####6     | 61/132 [00:08<00:08,  8.67it/s] 47%|####6     | 62/132 [00:08<00:07,  8.81it/s] 48%|####7     | 63/132 [00:08<00:08,  8.55it/s] 48%|####8     | 64/132 [00:08<00:07,  8.72it/s] 49%|####9     | 65/132 [00:08<00:07,  8.48it/s] 50%|#####     | 66/132 [00:08<00:07,  8.67it/s] 51%|#####     | 67/132 [00:09<00:07,  8.81it/s] 52%|#####1    | 68/132 [00:09<00:07,  8.55it/s] 52%|#####2    | 69/132 [00:09<00:07,  8.72it/s] 53%|#####3    | 70/132 [00:09<00:07,  8.84it/s] 54%|#####3    | 71/132 [00:09<00:07,  8.57it/s] 55%|#####4    | 72/132 [00:09<00:06,  8.74it/s] 55%|#####5    | 73/132 [00:09<00:06,  8.50it/s] 56%|#####6    | 74/132 [00:09<00:06,  8.68it/s] 57%|#####6    | 75/132 [00:09<00:06,  8.82it/s] 58%|#####7    | 76/132 [00:10<00:06,  8.56it/s] 58%|#####8    | 77/132 [00:10<00:06,  8.71it/s] 59%|#####9    | 78/132 [00:10<00:06,  8.84it/s] 60%|#####9    | 79/132 [00:10<00:06,  8.57it/s] 61%|######    | 80/132 [00:10<00:05,  8.73it/s] 61%|######1   | 81/132 [00:10<00:06,  8.50it/s] 62%|######2   | 82/132 [00:10<00:05,  8.68it/s] 63%|######2   | 83/132 [00:10<00:05,  8.82it/s] 64%|######3   | 84/132 [00:10<00:05,  8.55it/s] 64%|######4   | 85/132 [00:11<00:05,  8.72it/s] 65%|######5   | 86/132 [00:11<00:05,  8.85it/s] 66%|######5   | 87/132 [00:11<00:05,  8.57it/s]{'loss': 1.3919, 'learning_rate': 1.76e-07, 'epoch': 2.0}
                                                 67%|######6   | 88/132 [00:11<00:05,  8.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, label_class, text, __index_level_0__. If id, label_class, text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 150
  Batch size = 8

  0%|          | 0/19 [00:00<?, ?it/s][A
 21%|##1       | 4/19 [00:00<00:00, 36.42it/s][A
 42%|####2     | 8/19 [00:00<00:00, 29.17it/s][A
 63%|######3   | 12/19 [00:00<00:00, 28.84it/s][A
 79%|#######8  | 15/19 [00:00<00:00, 28.36it/s][A
 95%|#########4| 18/19 [00:00<00:00, 28.06it/s][A                                                
                                               [A{'eval_loss': 1.3994317054748535, 'eval_precision': 0.15645483043602165, 'eval_recall': 0.306959706959707, 'eval_accuracy': 0.29333333333333333, 'eval_f1': 0.18250377073906487, 'eval_runtime': 0.7035, 'eval_samples_per_second': 213.212, 'eval_steps_per_second': 27.007, 'epoch': 2.0}
 67%|######6   | 88/132 [00:12<00:05,  8.57it/s]
100%|##########| 19/19 [00:00<00:00, 28.06it/s][A
                                               [A 67%|######7   | 89/132 [00:12<00:11,  3.67it/s] 68%|######8   | 90/132 [00:12<00:09,  4.24it/s] 69%|######8   | 91/132 [00:12<00:08,  4.93it/s] 70%|######9   | 92/132 [00:12<00:07,  5.51it/s] 70%|#######   | 93/132 [00:12<00:06,  6.20it/s] 71%|#######1  | 94/132 [00:12<00:05,  6.83it/s] 72%|#######1  | 95/132 [00:12<00:05,  7.12it/s] 73%|#######2  | 96/132 [00:13<00:04,  7.62it/s] 73%|#######3  | 97/132 [00:13<00:04,  7.73it/s] 74%|#######4  | 98/132 [00:13<00:04,  8.10it/s] 75%|#######5  | 99/132 [00:13<00:04,  8.07it/s] 76%|#######5  | 100/132 [00:13<00:03,  8.36it/s] 77%|#######6  | 101/132 [00:13<00:03,  8.25it/s] 77%|#######7  | 102/132 [00:13<00:03,  8.50it/s] 78%|#######8  | 103/132 [00:13<00:03,  8.67it/s] 79%|#######8  | 104/132 [00:13<00:03,  8.46it/s] 80%|#######9  | 105/132 [00:14<00:03,  8.65it/s] 80%|########  | 106/132 [00:14<00:03,  8.45it/s] 81%|########1 | 107/132 [00:14<00:02,  8.64it/s] 82%|########1 | 108/132 [00:14<00:02,  8.44it/s] 83%|########2 | 109/132 [00:14<00:02,  8.64it/s] 83%|########3 | 110/132 [00:14<00:02,  8.79it/s] 84%|########4 | 111/132 [00:14<00:02,  8.53it/s] 85%|########4 | 112/132 [00:14<00:02,  8.71it/s] 86%|########5 | 113/132 [00:15<00:02,  8.48it/s] 86%|########6 | 114/132 [00:15<00:02,  8.67it/s] 87%|########7 | 115/132 [00:15<00:01,  8.81it/s] 88%|########7 | 116/132 [00:15<00:01,  8.55it/s] 89%|########8 | 117/132 [00:15<00:01,  8.72it/s] 89%|########9 | 118/132 [00:15<00:01,  8.48it/s] 90%|######### | 119/132 [00:15<00:01,  8.67it/s] 91%|######### | 120/132 [00:15<00:01,  8.81it/s] 92%|#########1| 121/132 [00:15<00:01,  8.55it/s] 92%|#########2| 122/132 [00:16<00:01,  8.72it/s] 93%|#########3| 123/132 [00:16<00:01,  8.49it/s] 94%|#########3| 124/132 [00:16<00:00,  8.68it/s] 95%|#########4| 125/132 [00:16<00:00,  8.81it/s] 95%|#########5| 126/132 [00:16<00:00,  8.55it/s] 96%|#########6| 127/132 [00:16<00:00,  8.72it/s] 97%|#########6| 128/132 [00:16<00:00,  8.84it/s] 98%|#########7| 129/132 [00:16<00:00,  8.57it/s] 98%|#########8| 130/132 [00:16<00:00,  8.73it/s] 99%|#########9| 131/132 [00:17<00:00,  8.50it/s]                                                 {'loss': 1.4008, 'learning_rate': 2.64e-07, 'epoch': 3.0}100%|##########| 132/132 [00:17<00:00,  8.50it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, label_class, text, __index_level_0__. If id, label_class, text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 150
  Batch size = 8

  0%|          | 0/19 [00:00<?, ?it/s][A
 21%|##1       | 4/19 [00:00<00:00, 36.58it/s][A
 42%|####2     | 8/19 [00:00<00:00, 29.21it/s][A
 63%|######3   | 12/19 [00:00<00:00, 28.86it/s][A
 79%|#######8  | 15/19 [00:00<00:00, 28.37it/s][A
 95%|#########4| 18/19 [00:00<00:00, 28.06it/s][A                                                 
                                               [A{'eval_loss': 1.397247552871704, 'eval_precision': 0.15645483043602165, 'eval_recall': 0.306959706959707, 'eval_accuracy': 0.29333333333333333, 'eval_f1': 0.18250377073906487, 'eval_runtime': 0.7031, 'eval_samples_per_second': 213.353, 'eval_steps_per_second': 27.025, 'epoch': 3.0}
100%|##########| 132/132 [00:17<00:00,  8.50it/s]
100%|##########| 19/19 [00:00<00:00, 28.06it/s][A
                                               [A

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|##########| 132/132 [00:17<00:00,  8.50it/s]100%|##########| 132/132 [00:17<00:00,  7.37it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, label_class, text, __index_level_0__. If id, label_class, text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 150
  Batch size = 8
{'train_runtime': 17.9113, 'train_samples_per_second': 58.622, 'train_steps_per_second': 7.37, 'train_loss': 1.3969732053352124, 'epoch': 3.0}
  0%|          | 0/19 [00:00<?, ?it/s] 21%|##1       | 4/19 [00:00<00:00, 36.58it/s] 42%|####2     | 8/19 [00:00<00:00, 31.28it/s] 63%|######3   | 12/19 [00:00<00:00, 29.92it/s] 84%|########4 | 16/19 [00:00<00:00, 28.05it/s]100%|##########| 19/19 [00:00<00:00, 26.64it/s]
INFO: [root] ***** Final Eval *****
 {'eval_loss': 1.397247552871704, 'eval_precision': 0.15645483043602165, 'eval_recall': 0.306959706959707, 'eval_accuracy': 0.29333333333333333, 'eval_f1': 0.18250377073906487, 'eval_runtime': 0.7405, 'eval_samples_per_second': 202.559, 'eval_steps_per_second': 25.657, 'epoch': 3.0}
Saving model checkpoint to C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model
Configuration saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\config.json
Model weights saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\pytorch_model.bin
tokenizer config file saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\tokenizer_config.json
Special tokens file saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\special_tokens_map.json
INFO: [guild] running test: test label_strat=multiclass
Resolving file:test_data.csv
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] Detected label names: ['Auszubildende', 'Verschiedenes', 'Führungskräfte', 'Fach- und Arbeitskräfte']
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--precision\4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Mon Dec 19 10:49:43 2022) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--recall\e40e6e98d18ff3f210f4d0b26fa721bfaa80704b1fdf890fa551cfabf94fc185 (last modified on Mon Dec 19 10:49:44 2022) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--accuracy\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Dec 19 10:49:46 2022) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--f1\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Mon Dec 19 10:49:47 2022) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.
all_f1: 0.4406779661016949
all_recall: 0.325
all_precision: 0.6842105263157895
all_accuracy: 0.5875
tk_f1: 0.4406779661016949
tk_recall: 0.325
tk_precision: 0.6842105263157895
tk_accuracy: 0.5875
ba_f1: 0.5294117647058824
ba_recall: 0.45
ba_precision: 0.6428571428571429
ba_accuracy: 0.6
len_f1: 0.4
len_recall: 0.25
len_precision: 1.0
len_accuracy: 0.64
