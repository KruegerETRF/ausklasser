INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
INFO: [root] num_labels: 2
INFO: [root] Detected label names: ['Auszubildende', 'Sonstige Arbeitnehmer']
Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/1 [00:00<?, ?ba/s]  0%|          | 0/1 [00:00<?, ?ba/s]
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--precision\4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Mon Dec 19 10:49:43 2022) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--recall\e40e6e98d18ff3f210f4d0b26fa721bfaa80704b1fdf890fa551cfabf94fc185 (last modified on Mon Dec 19 10:49:44 2022) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--accuracy\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Dec 19 10:49:46 2022) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
WARNING: [evaluate.loading] Using the latest cached version of the module from C:\Users\Admin\.cache\huggingface\modules\evaluate_modules\metrics\evaluate-metric--f1\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974 (last modified on Mon Dec 19 10:49:47 2022) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.
The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id, label_class, __index_level_0__. If text, id, label_class, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 123
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 48
  Number of trainable parameters = 67007234
  0%|          | 0/48 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  2%|2         | 1/48 [00:00<00:27,  1.68it/s]  4%|4         | 2/48 [00:00<00:14,  3.14it/s]  6%|6         | 3/48 [00:00<00:10,  4.49it/s]  8%|8         | 4/48 [00:00<00:08,  5.43it/s] 10%|#         | 5/48 [00:01<00:08,  5.31it/s] 12%|#2        | 6/48 [00:01<00:06,  6.08it/s] 15%|#4        | 7/48 [00:01<00:06,  6.60it/s] 17%|#6        | 8/48 [00:01<00:05,  7.24it/s] 19%|#8        | 9/48 [00:01<00:05,  7.46it/s] 21%|##        | 10/48 [00:01<00:04,  7.91it/s] 23%|##2       | 11/48 [00:01<00:04,  7.94it/s] 25%|##5       | 12/48 [00:01<00:04,  7.96it/s] 27%|##7       | 13/48 [00:02<00:04,  8.28it/s] 29%|##9       | 14/48 [00:02<00:04,  8.20it/s] 31%|###1      | 15/48 [00:02<00:04,  8.14it/s]                                               {'loss': 0.5497, 'learning_rate': 6.666666666666667e-05, 'epoch': 1.0}
 33%|###3      | 16/48 [00:02<00:03,  8.14it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id, label_class, __index_level_0__. If text, id, label_class, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 53
  Batch size = 8

  0%|          | 0/7 [00:00<?, ?it/s][A
 57%|#####7    | 4/7 [00:00<00:00, 34.54it/s][A
                                             [A                                               {'eval_loss': 0.20709365606307983, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.2661, 'eval_samples_per_second': 199.166, 'eval_steps_per_second': 26.305, 'epoch': 1.0}

100%|##########| 7/7 [00:00<00:00, 34.54it/s][A 33%|###3      | 16/48 [00:02<00:03,  8.14it/s]
                                             [A 35%|###5      | 17/48 [00:02<00:05,  5.97it/s] 38%|###7      | 18/48 [00:02<00:04,  6.37it/s] 40%|###9      | 19/48 [00:03<00:04,  6.92it/s] 42%|####1     | 20/48 [00:03<00:03,  7.18it/s] 44%|####3     | 21/48 [00:03<00:03,  7.64it/s] 46%|####5     | 22/48 [00:03<00:03,  7.74it/s] 48%|####7     | 23/48 [00:03<00:03,  7.81it/s] 50%|#####     | 24/48 [00:03<00:03,  7.86it/s] 52%|#####2    | 25/48 [00:03<00:02,  7.90it/s] 54%|#####4    | 26/48 [00:03<00:02,  8.24it/s] 56%|#####6    | 27/48 [00:03<00:02,  8.16it/s] 58%|#####8    | 28/48 [00:04<00:02,  8.43it/s] 60%|######    | 29/48 [00:04<00:02,  8.30it/s] 62%|######2   | 30/48 [00:04<00:02,  8.53it/s] 65%|######4   | 31/48 [00:04<00:02,  8.37it/s]                                               {'loss': 0.0565, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}
 67%|######6   | 32/48 [00:04<00:01,  8.37it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id, label_class, __index_level_0__. If text, id, label_class, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 53
  Batch size = 8

  0%|          | 0/7 [00:00<?, ?it/s][A
 57%|#####7    | 4/7 [00:00<00:00, 36.57it/s][A
{'eval_loss': 0.0052919830195605755, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.2656, 'eval_samples_per_second': 199.564, 'eval_steps_per_second': 26.357, 'epoch': 2.0}                                             [A                                               

100%|##########| 7/7 [00:00<00:00, 36.57it/s][A 67%|######6   | 32/48 [00:04<00:01,  8.37it/s]
                                             [A 69%|######8   | 33/48 [00:04<00:02,  6.05it/s] 71%|#######   | 34/48 [00:05<00:02,  6.44it/s] 73%|#######2  | 35/48 [00:05<00:01,  6.98it/s] 75%|#######5  | 36/48 [00:05<00:01,  7.23it/s] 77%|#######7  | 37/48 [00:05<00:01,  7.42it/s] 79%|#######9  | 38/48 [00:05<00:01,  7.85it/s] 81%|########1 | 39/48 [00:05<00:01,  7.89it/s] 83%|########3 | 40/48 [00:05<00:00,  8.22it/s] 85%|########5 | 41/48 [00:05<00:00,  8.15it/s] 88%|########7 | 42/48 [00:05<00:00,  8.11it/s] 90%|########9 | 43/48 [00:06<00:00,  8.37it/s] 92%|#########1| 44/48 [00:06<00:00,  8.26it/s] 94%|#########3| 45/48 [00:06<00:00,  8.50it/s] 96%|#########5| 46/48 [00:06<00:00,  8.35it/s] 98%|#########7| 47/48 [00:06<00:00,  8.57it/s]{'loss': 0.0041, 'learning_rate': 0.0, 'epoch': 3.0}
                                               100%|##########| 48/48 [00:06<00:00,  8.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id, label_class, __index_level_0__. If text, id, label_class, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 53
  Batch size = 8

  0%|          | 0/7 [00:00<?, ?it/s][A
 57%|#####7    | 4/7 [00:00<00:00, 36.55it/s][A                                               {'eval_loss': 0.0027522239834070206, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.2505, 'eval_samples_per_second': 211.545, 'eval_steps_per_second': 27.94, 'epoch': 3.0}
                                             [A100%|##########| 48/48 [00:06<00:00,  8.57it/s]

100%|##########| 7/7 [00:00<00:00, 36.55it/s][A
                                             [A

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 6.8951, 'train_samples_per_second': 53.516, 'train_steps_per_second': 6.961, 'train_loss': 0.20342527842149138, 'epoch': 3.0}
100%|##########| 48/48 [00:06<00:00,  8.57it/s]100%|##########| 48/48 [00:06<00:00,  6.97it/s]
The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id, label_class, __index_level_0__. If text, id, label_class, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 53
  Batch size = 8
  0%|          | 0/7 [00:00<?, ?it/s] 57%|#####7    | 4/7 [00:00<00:00, 36.59it/s]100%|##########| 7/7 [00:00<00:00, 30.40it/s]
INFO: [root] ***** Final Eval *****
 {'eval_loss': 0.0027522239834070206, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.2537, 'eval_samples_per_second': 208.904, 'eval_steps_per_second': 27.591, 'epoch': 3.0}
Saving model checkpoint to C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model
Configuration saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\config.json
Model weights saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\pytorch_model.bin
tokenizer config file saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\tokenizer_config.json
Special tokens file saved in C:/Users/Admin/Desktop/dev/kk/azb_klassifizierer/experiment_saves/current_model\special_tokens_map.json
